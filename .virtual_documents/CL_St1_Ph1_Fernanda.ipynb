

















import os
import sys
import pandas as pd
import fitz  # PyMuPDF
import logging
from tqdm import tqdm
import shutil





input_directory = 'cl_st1_ph1_fernanda_folders'
output_directory = 'cl_st1_ph1_output'
log_filename = f"{output_directory}/cl_st1_ph1_fernanda.log"





# Check if the output directory already exists. If it does, do nothing. If it doesn't exist, create it.
if os.path.exists(output_directory):
    print('Output directory already exists.')
else:
    try:
        os.makedirs(output_directory)
        print('Output directory successfully created.')
    except OSError as e:
        print('Failed to create the directory:', e)
        sys.exit(1)





logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename = log_filename
)





# Initialising an empty list to hold the directory information
directory_data = []

# Walking through the directory structure
for root, dirs, files in os.walk(input_directory):
    #for file in tqdm(files, desc='Processing files'):
    for file in files:
        if file.endswith('.pdf'):
            try:
                # Getting the full file path
                file_path = os.path.join(root, file)
                # Splitting the root into individual directories
                directory_parts = root.split(os.sep)
                # Creating a dictionary for this file's information
                file_info = {'File': file}
                # Adding the file path to the dictionary
                file_info['File Path'] = file_path
                # Adding each part of the directory to the dictionary with appropriate keys
                for i, part in enumerate(directory_parts):
                    file_info[f"Directory Level {i+1}"] = part
                
                # Opening the PDF file
                document = fitz.open(file_path)
                # Extracting text from each page
                text = ''
                for page_num in range(document.page_count):
                    page = document[page_num]
                    text += page.get_text()

                file_info['Scraped Text'] = text
                document.close()
                
                # Adding the file info to the list
                directory_data.append(file_info)
                
                # Logging the successful extraction
                logging.info(f"Successfully scraped {file_path}")
            except Exception as e:
                # Logging any errors
                logging.error(f"Error scraping {file_path}: {str(e)}")

# Converting the list of dictionaries into a DataFrame
df = pd.DataFrame(directory_data)


df





df = df.rename(columns={'Directory Level 1': 'Root Directory', 'Directory Level 2': 'Document Type', 'Directory Level 3': 'Year', 'Directory Level 4': 'Branch', 'Directory Level 5': 'Unit'})


df





df['Text ID'] = 't' + df.index.astype(str).str.zfill(6)


df.dtypes





df[['Root Directory', 'Document Type', 'Year', 'Branch', 'Unit', 'File', 'File Path', 'Text ID', 'Scraped Text']].to_json(f"{output_directory}/cl_st1_ph1_fernanda_scraped.jsonl", orient='records', lines=True)





df = pd.read_json(f'{output_directory}/cl_st1_ph1_fernanda_scraped.jsonl', lines=True)


df.dtypes


df





df.at[21, 'File']


df.at[21, 'File Path']


df.at[21, 'Text ID']


df.at[21, 'Scraped Text']








# Find rows where the specified column has empty strings
mask = df['Scraped Text'].str.len() == 0

# Get the corresponding 'Text ID' values 
text_ids_with_missing_text = df[mask]['Text ID']

text_ids_with_missing_text





for index, row in df.iterrows():
    source_path = row['File Path']
    new_name = row['Text ID'] + '.pdf'
    destination_path = os.path.join(output_directory, new_name)
    try:
        shutil.copy(source_path, destination_path)
        logging.info(f"Copied: {source_path} to {destination_path}")
    except Exception as e:
        logging.error(f"Error copying {source_path}: {str(e)}")


























# Defining a function to tokenise the paragraphs of each article
def paragraph_tokenise(text):
    lines = text.split('\n')
    paragraphs = []
    paragraph = ''
    
    for line in lines:
        if line.strip():
            cleaned_line = ' '.join(line.split())  # Remove extra spaces within the line
            paragraph += ' ' + cleaned_line.strip()  # Join subsequent lines into a paragraph
        else:
            paragraphs.append(paragraph.strip())  # If there is an empty line, the paragraph consolidated so far is added to the list of paragraphs
            paragraph = ''  # The paragraph variable is cleared out
    
    if paragraph:
        paragraphs.append(paragraph.strip())  # The last paragraph is added to the list of paragraphs
    
    tokenised_paragraphs = '\n'.join(paragraphs)  # The list of paragraphs is compiled into a text with each paragraph as a separate line
    
    return tokenised_paragraphs

# Defining a function to read the content of a TXT file
def read_txt_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        logging.error(f"Error reading file {file_path}: {e}")
        return None

# Defining a function to save the paragraph-tokenised articles into TXT files
def save_paragraph_tokenised_file(output_text_content, output_file):
    try:
        with open(output_file, 'w', encoding='utf-8') as output_txt_file:
            output_txt_file.write(output_text_content)
        logging.info(f"Successfully saved tokenised file: {output_file}")
    except Exception as e:
        logging.error(f"Error saving file {output_file}: {e}")

# Iterating through each row in the DataFrame and add the text content
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc='Processing files'):
    text_id = row['Text ID']
    txt_file_path = os.path.join(output_directory, f"{text_id}.txt")
    if os.path.exists(txt_file_path):
        text_content = read_txt_file(txt_file_path)
        if text_content:
            paragraph_tokenised_text_content = paragraph_tokenise(text_content)
            save_paragraph_tokenised_file(paragraph_tokenised_text_content, f"{output_directory}/{text_id}_tokenised.txt")
    else:
        logging.warning(f"File not found: {txt_file_path}")





# Prepare to collect rows
data = []

# Loop through each 'Text ID' in df
for _, row in df.iterrows():
    text_id = row['Text ID']

    paragraph_count = 0
    section = None
    file_path = os.path.join(output_directory, f"{text_id}_tokenised.txt")

    if not os.path.isfile(file_path):
        print(f"Missing file: {file_path}")
        continue

    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            line = ' '.join(line.split()).strip()

            if line.startswith('Section:'):
                section_name = line.partition(':')[2].strip()
                # If a section line is blank or only has the colon, we gracefully assign the fallback name 'Undefined Section'
                section = section_name if section_name else 'Undefined Section'
                paragraph_count = 0  # Resetting paragraph count for new section

            elif line:
                paragraph_count += 1
                data.append({
                    'Text ID': text_id,
                    'Section': section,
                    'Paragraph': f"Paragraph {paragraph_count}",
                    'Text Paragraph': line
                })

# Create final DataFrame
df_section_paragraph = pd.DataFrame(data)


df_section_paragraph





df.drop(columns=['Scraped Text'], inplace=True)


df_fatec = df.merge(df_section_paragraph, on='Text ID', how='left')


df_fatec





print(df_fatec.at[17, 'Text Paragraph'])





df_fatec.to_json(f"{output_directory}/cl_st1_ph1_fernanda.jsonl", orient='records', lines=True)


df_fatec.to_excel(f"{output_directory}/cl_st1_ph1_fernanda.xlsx", index=False)



